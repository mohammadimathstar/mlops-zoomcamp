{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1983b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9019968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca833d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "        df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "    elif filename.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    df['PU_DO'] = df.PULocationID + \"_\" + df.DOLocationID\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594609c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54373 51497\n"
     ]
    }
   ],
   "source": [
    "df_train = read_dataframe('./data/green_tripdata_2024-01.parquet')\n",
    "df_val = read_dataframe('./data/green_tripdata_2024-02.parquet')\n",
    "\n",
    "print(df_train.shape[0], df_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d562f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PU_DO', 'PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']\n",
    "target = 'duration'\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa165bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root mean squared error is: 5.926355548541141\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(train_dicts, y_train)\n",
    "y_pred = pipeline.predict(val_dicts)\n",
    "\n",
    "print(f\"root mean squared error is: {root_mean_squared_error(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c61ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump(pipeline, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "\n",
    "# mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"mohammad\")\n",
    "\n",
    "    mlflow.log_param(\"train-data-path\", \"./data/green_tripdata_2024-01.parquet\")\n",
    "    mlflow.log_param(\"valid-data-path\", \"./data/green_tripdata_2024-02.parquet\")\n",
    "\n",
    "    pipeline = make_pipeline(\n",
    "        DictVectorizer(),\n",
    "        Lasso(alpha)\n",
    "    )\n",
    "    \n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "    pipeline.fit(train_dicts, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(val_dicts)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"./models/lin_reg.bin\", artifact_path=\"models_pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82d9b4",
   "metadata": {},
   "source": [
    "### Adding hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc204ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# !pip install hyperopt\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# the following uses some 'baysian approach' to find the best hyper-parameter\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "244a3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dv = DictVectorizer()\n",
    "# X_train = dv.fit_transform(train_dicts)\n",
    "# X_val = dv.fit_transform(val_dicts)\n",
    "\n",
    "# train = xgb.DMatrix(X_train, label=y_train)\n",
    "# valid = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "395b7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"lasso\")\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        pipeline = make_pipeline(\n",
    "            DictVectorizer(),\n",
    "            Lasso(alpha=params['alpha'])\n",
    "        )\n",
    "        pipeline.fit(train_dicts, y_train)\n",
    "        y_pred = pipeline.predict(val_dicts)\n",
    "\n",
    "        y_pred = pipeline.predict(val_dicts)\n",
    "        rmse = root_mean_squared_error(y_val, y_pred)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19472db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:07<00:00,  6.39s/trial, best loss: 7.406882414959407]\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    'alpha': hp.loguniform('alpha', -5, -1),\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(params):\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    seed = params.get('seed', 42)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            # params,\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_depth=int(params['max_depth']),\n",
    "            reg_alpha=params['reg_alpha'],\n",
    "            reg_lambda=params['reg_lambda'],\n",
    "            random_state=seed,\n",
    "            verbosity=0,\n",
    "            objective=params['objective']\n",
    "        )\n",
    "\n",
    "        pipeline = make_pipeline(\n",
    "            DictVectorizer(),\n",
    "            model\n",
    "        )\n",
    "\n",
    "        pipeline.fit(train_dicts, y_train)\n",
    "        y_pred = pipeline.predict(val_dicts)\n",
    "        rmse = root_mean_squared_error(y_val, y_pred)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4a5e5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [11:09<1:01:32, 230.77s/trial, best loss: 5.145984609368992]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -6, -1),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:squarederror',\n",
    "    'seed': 42\n",
    "}\n",
    "best_result = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06cbde84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/14 11:26:58 WARNING mlflow.sklearn: Unrecognized dataset type <class 'list'>. Dataset logging skipped.\n",
      "2025/05/14 11:27:05 WARNING mlflow.sklearn: Unrecognized dataset type <class 'list'>. Dataset logging skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.112587189508359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/14 11:27:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "\n",
    "\n",
    "params = {\n",
    "    'learning_rate':    0.410215639489084,\n",
    "    'max_depth':    9,\n",
    "    'min_child_weight':    2.955225015279242,\n",
    "    'objective':     'reg:squarederror',\n",
    "    'reg_alpha':    0.07847140786574915,\n",
    "    'reg_lambda':    0.15098298624710604,\n",
    "    'seed':    42\n",
    "}\n",
    "\n",
    "model = XGBRegressor(\n",
    "    learning_rate=params['learning_rate'],\n",
    "    max_depth=int(params['max_depth']),\n",
    "    reg_alpha=params['reg_alpha'],\n",
    "    reg_lambda=params['reg_lambda'],\n",
    "    random_state=params['seed'],\n",
    "    verbosity=0,\n",
    "    objective=params['objective']\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    model\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    pipeline.fit(train_dicts, y_train)\n",
    "    y_pred = pipeline.predict(val_dicts)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "    print(rmse)\n",
    "\n",
    "    mlflow.set_tag(\"model\", \"xgboost\")\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    # Log the entire sklearn pipeline\n",
    "    mlflow.sklearn.log_model(pipeline, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b87645",
   "metadata": {},
   "source": [
    "# TO Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9b121f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/14 11:39:01 WARNING mlflow.sklearn: Unrecognized dataset type <class 'list'>. Dataset logging skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.112587189508359\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "# Load the pipeline model\n",
    "model_uri = 'runs:/65a7096112cb41b693e557e8008cfa9c/model'\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Predict directly on val_dicts (list of dicts)\n",
    "preds = model.predict(val_dicts)\n",
    "\n",
    "print(root_mean_squared_error(y_val, preds))\n",
    "\n",
    "# print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c50756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
